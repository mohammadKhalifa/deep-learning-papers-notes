## [Transformer-XL: Attentive Language Models
Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf)


## Notes
* One major problem with the [Transformer](https://arxiv.org/abs/1706.03762) architecture is that they operate on **Fixed-length** context, that is the input sequence length (typically 512 tokens) is always fixed.
* This paper proposes two novel ideas to remedy such problem:

1. *Recurrence Mechanism* : 

