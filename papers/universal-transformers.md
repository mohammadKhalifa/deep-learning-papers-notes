## [Universal Transformers](https://arxiv.org/abs/1807.03819)

* Transfomers are self-attentive models proposed originally [here](https://arxiv.org/abs/1706.03762) to address a significant shortcoming of RNNs, namely their inherently sequential computation which prevents parallelization across elements of the input sequence.



### What they did
* 


### Insight
* 

### Drawbacks and Possible Improvements
* 
