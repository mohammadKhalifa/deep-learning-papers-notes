## paper-notes

The idea of this repo was inspired by Jason Benn's similar [repo](https://github.com/JasonBenn/deep-learning-paper-notes/).

#### 2018-07
* Difficulty Controllable Question Generation for Reading Comprehension [[notes](papers/dc-question-generation.md)] [[link](https://arxiv.org/abs/1807.03586)]
*  Universal Transformers [[notes](papers/universal-transformers.md)] [[link](https://arxiv.org/abs/1807.03819)]
* Layer Normalization [[Notes](papers/layer-normalization.md)] [[link](https://arxiv.org/abs/1607.06450)]
* Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift [[Notes](papers/batch-normalization.md)] [[link](https://arxiv.org/abs/1502.03167)]
* Bilateral Multi-Perspective Matching for Natural Language Sentences [[Link](https://arxiv.org/abs/1702.03814)] [[Notes](papers/bilateral-matching.md)]
* Deep Contextualized Word Representations [[notes](papers/elmo.md)]
* A Model To Learn Them All [[notes](papers/model-all.md)]
* Neural Architecture Search with Reinforcement Learning. [[notes](papers/rl-search.md)]

#### 2018-08
* Asynchronous Methods for Deep Reinforcement Learning [[notes](papers/async-rl.md)]
* Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting [[notes](papers/fast-abs.md)]
* Improving Abstraction in Text Summarization [[notes](papers/improv-abs.md)]



#### 2019-02

* Learning Unsupervised Learning Rules  [[notes](papers/unsupr-rules.md)]

#### 2019-06
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [[notes](papers/xl-transformer.md)]
* XLNet: Generalized Autoregressive Pretraining for Language Understanding [[notes](papers/xlnet.md)]



